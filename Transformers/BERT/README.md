# BERT (Bidirectional Encoder Representations from Transformers)

### General Characteristics about BERT
* Uses only the **Encoder mechanism** of traditional transformer models.  
* Contrary to directional models (read text input sequentially), BERT is bidirectional meaning that the transformer **reads an entire sequence of words at once!**
