{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOxaKG6FVcL3OiIBhmTgCoo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tristanoprofetto/neural-networks/blob/main/GAN/StyleGAN/StyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOY4Uu-G2jt1"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import truncnorm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Ss-3Ay2oan"
      },
      "source": [
        "# Function for visualizing images given a tensor\n",
        "def show_images(tensor, n, size=(3, 64, 64), nrow=3):\n",
        "\n",
        "  image = (tensor+1)/2\n",
        "  image = image.detach().cpu().clamp_(0, 1)\n",
        "  grid = make_grid(image[:n], nrow=nrow, padding=0)\n",
        "\n",
        "  plt.imshow(grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-lP2BoJ3RpT"
      },
      "source": [
        "### Truncation Trick\n",
        "\n",
        "The truncation trick resamples the noise vector $z$ from a truncated normal distribution which allows you to tune the generator's fidelity/diversity. The truncation value is at least 0, where 1 means there is little truncation (high diversity) and 0 means the distribution is all truncated except for the mean (high quality/fidelity). This trick is not exclusive to StyleGAN. In fact, you may recall playing with it in an earlier GAN notebook.\n",
        "\n",
        "* Truncation: non-negative scalar\n",
        "* Z Dim: dimension of the noise vector\n",
        "* Number of Samples: total samples to be generated\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex42hw4j3Zvq"
      },
      "source": [
        "def truncation_noise(n_samples, z_dim, truncation):\n",
        "  z = truncnorm.rvs(-1 * truncation, truncation, size=(n_samples, z_dim))\n",
        "  z = torch.Tensor(z)\n",
        "\n",
        "  return z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "838nW717DHv8",
        "outputId": "227b5ad8-5a9f-4ad8-c7db-e018a3529234"
      },
      "source": [
        "# Unit Test\n",
        "assert tuple(truncation_noise(5, 10, 0.7).shape) ==  (10, 5)\n",
        "simple_noise = truncation_noise(10, 1000, truncation=0.2)\n",
        "assert simple_noise.max() > 0.199 and simple_noise.max() < 2\n",
        "assert simple_noise.min() < -0.199 and simple_noise.min() > -0.2\n",
        "assert simple_noise.std() > 0.113 and simple_noise.std() < 0.117\n",
        "print(\"Success!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG1WOGES4GNT"
      },
      "source": [
        "### Mapping Network\n",
        "\n",
        "takes the noise vector, $z$, and maps it to an intermediate noise vector, $w$. This makes it so $z$ can be represented in a more disentangled space which makes the features easier to control later.\n",
        "\n",
        "(The mapping network in StyleGAN is composed of 8 layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkvjOv1y4TFA"
      },
      "source": [
        "class MappingNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self, z_dim, w_dim, hidden_dim):\n",
        "    super().__init__()\n",
        "    self.mapping = nn.Sequential(\n",
        "        nn.Linear(z_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, w_dim)\n",
        "    )\n",
        "\n",
        "  # Function for completing the forward pass of the Mapping Network given 'z', returns 'w'\n",
        "  def feedForward(self, z):\n",
        "    w = self.mapping(z)\n",
        "    return w\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaSkmkfF51QU"
      },
      "source": [
        "### Random Noise Injection\n",
        "\n",
        "Noise tensor is initialized as one random channel, then multiplied by learned weights for each channel in the image ... noise tensor must be the same size as the feature map ... (Occurs before every AdaIN block)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qvviVJO6ddt"
      },
      "source": [
        "class NoiseInjection(nn.Module):\n",
        "\n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(channels)[None, :, None, None])\n",
        "\n",
        "  # Forwards pass: given an image adds random noise\n",
        "  def feedForward(self, image):\n",
        "    z = torch.randn(image.shape[0], 1, image.shape[2], iamge.shape[3])\n",
        "    image = image + self.weights * z\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0FtAi4a8YoN"
      },
      "source": [
        "### Adaptive Instance Normalization (AdaIN)\n",
        "\n",
        "By injecting $w$ (intermediate noise) mutliple times throughout the network ... \n",
        "AdaIN takes the instance normalization of the image and multiplies it by the style scale ($y_s$) and adds the style bias ($y_b$). You need to calculate the learnable style scale and bias by using linear mappings from $w$.\n",
        "\n",
        "$ \\text{AdaIN}(\\boldsymbol{\\mathrm{x}}_i, \\boldsymbol{\\mathrm{y}}) = \\boldsymbol{\\mathrm{y}}_{s,i} \\frac{\\boldsymbol{\\mathrm{x}}_i - \\mu(\\boldsymbol{\\mathrm{x}}_i)}{\\sigma(\\boldsymbol{\\mathrm{x}}_i)} + \\boldsymbol{\\mathrm{y}}_{b,i} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2jliEMB87x9"
      },
      "source": [
        "class AdaIN(nn.Module):\n",
        "\n",
        "  def __init__(self, w_dim, channels):\n",
        "    super().__init__()\n",
        "\n",
        "    self.instanceNorm = nn.InstanceNorm2d(channels)\n",
        "    self.styleScaleTransform = nn.Linear(w_dim, channels)\n",
        "    self.styleShiftTransform = nn.Linear(w_dim, channels)\n",
        "\n",
        "  # Given an image and w, returns the normalized image that is scaled + shifted by the style\n",
        "  def feedForward(self, image, w):\n",
        "\n",
        "    norm_image = self.instanceNorm(image)\n",
        "    scale = self.styleScaleTransform(w)[:, :, None, None]\n",
        "    shift = self.styleShiftTransform(w)[:, :, None, None]\n",
        "\n",
        "    final_image = norm_image * scale + shifted\n",
        "\n",
        "    return final_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb4Fu5hx-r1Z"
      },
      "source": [
        "### Progressive Growing\n",
        "\n",
        "This component helps StyleGAN create high resolution images by doubling the images' size gradually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8r2psKj-76W"
      },
      "source": [
        "class GeneratorBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, w_dim, inputChannels, outputChannels, kernel_size, initial_size,upsample=True):\n",
        "    super().__init__()\n",
        "\n",
        "    self.upsample = upsample\n",
        "    if self.upsample:\n",
        "      self.upsample = nn.Upsample((initial_size), mode='bilinear')\n",
        "\n",
        "    self.c = nn.Conv2d(inputChannels, outputChannels, kernel_size, padding=1)\n",
        "    self.z = NoiseInjection(outputChannels)\n",
        "    self.AdaIN = AdaIN(outputChannels, w_dim)\n",
        "    self.activation = nn.LeakyReLU(0.2)\n",
        "\n",
        "  # Given x and w, returns a StyleGAN generator block\n",
        "  def feedForward(self, x, w):\n",
        "\n",
        "    if self.upsample:\n",
        "      x = self.upsample(x)\n",
        "\n",
        "    x = self.c(x)\n",
        "    x = self.z(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.AdaIN(x, w)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC_LUjwoERua"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "  def __init__(self, z_dim, w_dim, hidden_dim, inputChannels, outputChannels, hiddenChannels, kernel_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.map = MappingNetwork(z_dim, w_dim, hidden_dim)\n",
        "    self.constant = nn.Parameter(torch.randn(1, inputChannels, 4, 4))\n",
        "\n",
        "    self.b0 = GeneratorBlock(w_dim, inputChannels, outputChannels, kernel_size, 4, upsample=False)\n",
        "    self.b1 = GeneratorBlock(w_dim, hiddenChannels, hiddenChannels, kernel_size, 8)\n",
        "    self.b2 = GeneratorBlock(w_dim, hiddenChannels, hiddenChannels, kernel_size, 16)\n",
        "\n",
        "    self.b1_2_image = nn.Conv2d(hiddenChannels, outputChannels, kernel_size=1)\n",
        "    self.b2_2_image = nn.Conv2d(hiddenChannels, outputChannels, kernel_size=1)\n",
        "\n",
        "    self.alpha = 0.2\n",
        "\n",
        "\n",
        "  # Upsampling small images to big images\n",
        "  def Upsample(self, small, big):\n",
        "    return F.interpolate(small, size=big.shape[-2:], mode='bilinear')\n",
        "\n",
        "  \n",
        "  def feedForward(self, z, return_intermediate=False):\n",
        "\n",
        "    x = self.constant\n",
        "    w = self.map(z)\n",
        "    x = self.b0(x, w)\n",
        "\n",
        "    x_small = self.b1(x, w)\n",
        "    small_image = self.b1_2_image(x_small)\n",
        "\n",
        "    x_big = self.b2(x_small, w)\n",
        "    big_image = self.b2_2_image(x_big)\n",
        "\n",
        "    x_upsample = self.Upsample(small_image, big_image)\n",
        "\n",
        "\n",
        "    interpolation = self.alpha * (big_image) + (1-self.alpha) * (x_upsample)\n",
        "\n",
        "    if return_intermediate:\n",
        "      return interpolation, x_upsample, big_image\n",
        "    return interpolation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "C36AL5YWHgig",
        "outputId": "59d2fecc-49bb-4c7b-a19d-af90089166b7"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [15, 15]\n",
        "\n",
        "z_dim = 128\n",
        "w_dim = 496\n",
        "hidden_dim = 1024\n",
        "inputChannels = 512\n",
        "outputChannels = 3\n",
        "kernel_size=3\n",
        "hiddenChannels = 256\n",
        "truncation = 0.7\n",
        "\n",
        "vZ = truncation_noise(10, z_dim, truncation) * 10\n",
        "\n",
        "stylegan = Generator(z_dim, w_dim, hidden_dim, inputChannels, outputChannels, hiddenChannels, kernel_size)\n",
        "\n",
        "stylegan.eval()\n",
        "images= []\n",
        "for alpha in np.linspace(0, 1, num=5):\n",
        "  stylegan.alpha = alpha\n",
        "  result,_,_ = stylegan(vZ, return_intermediate=True)\n",
        "  images += [tensor for tensor in result]\n",
        "\n",
        "show_images(torch.stack(images), nrow=10, n=len(images))\n",
        "stylegan = stylegan.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-3b82671b0f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mstylegan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstylegan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_intermediate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mimages\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _forward_unimplemented() got an unexpected keyword argument 'return_intermediate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpRV7TQwJ0Qc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}