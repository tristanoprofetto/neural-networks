{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHfCBtf3RmxI7E2ZlOwUUS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tristanoprofetto/neural-networks/blob/main/CNN/U-Net/U_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E5zP29K3IIE"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)\n",
        "import torch.nn.functional as F\n",
        "from skimage import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WG3fnxc3SaW"
      },
      "source": [
        "def show_image(images, n_images, size=(1, 28, 28)):\n",
        "\n",
        "  unflat = images.detach().cpu().view(-1, *size)\n",
        "  grid = make_grid(unflat[:n_images], nrow=4)\n",
        "  plt.imshow(grid.permute(1, 2, 0).squeeze())\n",
        "  plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnI-vMPb4_Ms"
      },
      "source": [
        "# Function for cropping an image given a tensor and the desired shape\n",
        "def crop(image, new_size):\n",
        "\n",
        "  h = image.shape[2] //2 # Height\n",
        "  w = image.shape[3] //2 # Width\n",
        "\n",
        "  start_h = h - new_size[2] //2\n",
        "  start_w = w - new_size[3] //2\n",
        "\n",
        "  final_h = start_h + new_size[2]\n",
        "  final_w = start_w + new_size[3]\n",
        "\n",
        "  cropped = image[:, :, start_h:final_h, start_w:final_w]\n",
        "\n",
        "  return cropped"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jwKj5KQ3-Jv"
      },
      "source": [
        "class ContractingBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, inputChannels):\n",
        "    super(ContractingBlock, self).__init__()\n",
        "\n",
        "    self.c1 = nn.Conv2d(inputChannels, 2*inputChannels, kernel_size=3)\n",
        "    self.c2 = nn.Conv2d(2*inputChannels, 2*inputChannels, kernel_size=3)\n",
        "    self.activation = nn.ReLU()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "  \n",
        "  def feedForward(self, x):\n",
        "\n",
        "    x = self.c1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.c2(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npX0ZNa94v5W"
      },
      "source": [
        "class ExpandingBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, inputChannels):\n",
        "    super(ExpandingBlock, self).__init__()\n",
        "\n",
        "    self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "    self.c1 = nn.Conv2d(inputChannels, inputChannels // 2, kernel_size=2, stride=1)\n",
        "    self.c2 = nn.Conv2d(inputChannels, inputChannels //2, kernel_size=3, stride=1)\n",
        "    self.c3 = nn.Conv2d(inputChannels //2, inputChannels //2, kernel_size=3, stride=1)\n",
        "\n",
        "    self.activation = nn.ReLU()\n",
        "\n",
        "\n",
        "  def feedForward(self, x, skip_connection):\n",
        "\n",
        "    x = self.upsample(x)\n",
        "    x = self.c1(x)\n",
        "    \n",
        "    skip_connection = crop(skip_connection, x.shape)\n",
        "    x = torch.cat([x, skip_connection], axis=1)\n",
        "\n",
        "    x = self.c2(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.c3(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ii680r7Whe"
      },
      "source": [
        "class FeatureMapBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, inputChannels, outputChannels):\n",
        "    super(FeatureMapBlock, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(inputChannels, outputChannels, kernel_size=1)\n",
        "\n",
        "\n",
        "  def feedForward(self, x):\n",
        "\n",
        "    x= self.conv(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtX-ciw972af"
      },
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "  def __init__(self, inputChannels, outpuChannels, hiddenChannels=64):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    self.upfeature = nn.FeatureMapBlock(inputChannels, hiddenChannels)\n",
        "    \n",
        "    self.c1 = ContractingBlock(hiddenChannels)\n",
        "    self.c2 = ContractingBlock(2*hiddenChannels)\n",
        "    self.c3 = ContractingBlock(4*hiddenChannels)\n",
        "    self.c4 = ContractingBlock(8*hiddenChannels)\n",
        "\n",
        "    self.e1 = ExpandingBlock(16*hiddenChannels)\n",
        "    self.e2 = ExpandingBlock(8*hiddenChannels)\n",
        "    self.e3 = ExpandingBlock(4*hiddenChannels)\n",
        "    self.e4 = ExpandingBlock(2*hiddenChannels)\n",
        "\n",
        "    self.downfeature = FeatureMapBlock(hiddenChannels, outputChannels)\n",
        "\n",
        "\n",
        "  def feedForward(self, x):\n",
        "\n",
        "    x0 = self.upfeature(x)\n",
        "    x1 = self.c1(x0)\n",
        "    x2 = self.c2(x1)\n",
        "    x3 = self.c3(x2)\n",
        "    x4 = self.c4(x3)\n",
        "\n",
        "    x5 = self.e1(x4, x4)\n",
        "    x6 = self.e2(x5, x2)\n",
        "    x7 = self.e3(x6, x1)\n",
        "    x8 = self.e4(x7, x0)\n",
        "\n",
        "    x = self.downfeature(x8)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVoVXu99dP1"
      },
      "source": [
        "### Training \n",
        "\n",
        "* criterion: the loss function\n",
        "* epochs: the number of training steps\n",
        "* input_dim: the number of channels of the input image\n",
        "* label_dim: the number of channels of the output image\n",
        "* display_step: how often to display/visualize the images\n",
        "* batch_size: the number of images per forward/backward pass\n",
        "* lr: the learning rate\n",
        "* initial_shape: the size of the input image (in pixels)\n",
        "* target_shape: the size of the output image (in pixels)\n",
        "* device: gpu or cpu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0jRyxHy9YmC"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "epochs = 200\n",
        "input_dim = 1\n",
        "label_dim = 1\n",
        "display_step = 20\n",
        "batch_size = 4\n",
        "lr = 0.0002\n",
        "initial_shape = 512\n",
        "target_shape = 373\n",
        "device = 'cuda'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeBWLtxA-ANM"
      },
      "source": [
        "volumes = torch.Tensor(io.imread('/train'))[:, None, :, :] / 255\n",
        "labels = torch.Tensor(io.imread('/labels.tif', plugin=\"tifffile\"))[:, None, :, :] / 255\n",
        "labels = crop(labels, torch.Size([len(labels), 1, target_shape, target_shape]))\n",
        "dataset = torch.utils.data.TensorDataset(volumes, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omvSvRLY-tvp"
      },
      "source": [
        "def train():\n",
        "  data = DataLoader(dataset, batch_size, shuffle=True)\n",
        "\n",
        "  model = UNet(input_dim, label_dim).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameter(), lr=lr)\n",
        "  current_step = 0\n",
        "\n",
        "  for i in range(0, epochs):\n",
        "    for real, labels in tqdm(data):\n",
        "      current_batch = len(real)\n",
        "\n",
        "      real = real.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      predictions = model(real)\n",
        "      loss = criterion(predictions, labels)\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      if current_step % display_step == 0:\n",
        "\n",
        "        print(f\"Epoch {epoch}: Step {current_step}: Model Loss: {loss.item.item()}\")\n",
        "\n",
        "        show_image(crop(real, torch.Size([len(real), 1, target_shape])), size=(input_dim, target_shape, target_shape))\n",
        "        show_image(labels, size=(label_dim, target_shape, target_shape))\n",
        "        show_image(torch.sigmoid(predictions), size=(label_dim, target_shape, target_shape))\n",
        "\n",
        "      current_step += 1\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}